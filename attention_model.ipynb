{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, RepeatVector, multiply\n",
    "from keras.layers import Bidirectional, LSTM, MaxPooling1D, Embedding, GRU\n",
    "from keras.layers import Flatten, Softmax, Activation, Lambda\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.optimizers import Adam\n",
    "import fasttext\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = fasttext.train_unsupervised(\n",
    "    '/Users/Masters/Documents/repos/sentiment_analysis/corpus.txt',\n",
    "    model='skipgram',\n",
    "    dim = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_model(\"fast_text_sentiment_20d.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.load_model(\"fast_text_sentiment_20d.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = '/Users/Masters/Downloads/'\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NUM_WORDS = 5000\n",
    "EMBEDDING_DIM = 20\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/Masters/Downloads/train_F3WbcTw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lens = [len(tx.split(' ')) for tx in df['text'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[(df['sentiment'] == 0) | (df['sentiment'] == 1)]\n",
    "df = df[df['drug'].isin(['gilenya', 'ocrevus', 'ocrelizumab', 'fingolimod',\n",
    "                        'opdivo', 'remicade', 'humira', 'entyvio',\n",
    "                        'tarceva', 'cladribine', 'keytruda', 'stelara', 'tagrisso',\n",
    "                        'alimta'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_weights = class_weight.compute_sample_weight('balanced',\n",
    "                                                    df['sentiment'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_dict = {2: 0.5, 1: 5.0, 0:10}\n",
    "sample_weights = np.array([weights_dict[w] for w in df['sentiment'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.5, 10.0, 5.0}\n",
      "[ 0.5  0.5  0.5 ...  0.5  0.5 10. ] [2 2 2 ... 2 2 0]\n",
      "2    2915\n",
      "1     734\n",
      "0     483\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(set(sample_weights))\n",
    "print(sample_weights, df['sentiment'].values)\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34603 unique tokens.\n",
      "Shape of data tensor: (4132, 200)\n",
      "Shape of label tensor: (4132, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4132, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(df['text'].values+df['drug'].values)\n",
    "sequences = tokenizer.texts_to_sequences(df['text'].values)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(df['sentiment']))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "drug_sequences = tokenizer.texts_to_sequences(df['drug'].values)\n",
    "drug_sequences = np.array(drug_sequences)\n",
    "drug_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['drug'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "random.seed(9001)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "drug_sequences = drug_sequences[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "q_train = drug_sequences[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "q_val = drug_sequences[-num_validation_samples:]\n",
    "sample_weights_train = sample_weights[:-num_validation_samples]\n",
    "sample_weights_val = sample_weights[-num_validation_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = model[word]\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "query_input = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "embedded_query = embedding_layer(query_input)\n",
    "embedded_query = Flatten()(embedded_query)\n",
    "\n",
    "x = Bidirectional(GRU(10, activation='relu', return_sequences=True))\\\n",
    "                            (embedded_sequences)\n",
    "embedded_query = RepeatVector(MAX_SEQUENCE_LENGTH)(embedded_query)\n",
    "multiplied = multiply([embedded_query, x])\n",
    "output = Dense(1, activation='relu', kernel_regularizer='l2')(multiplied)\n",
    "output = Flatten()(output)\n",
    "output = Activation('softmax')(output)\n",
    "\n",
    "output = Dense(2, activation='softmax')(output)\n",
    "\n",
    "model = Model(inputs=[sequence_input, query_input], outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dot_prod(var):\n",
    "    return K.sum(var[0] * var[1],axis=-1,keepdims=True)\n",
    "\n",
    "def weighted_sum(var):\n",
    "    context_vector = var[0] * var[1]\n",
    "    return tf.reduce_sum(context_vector, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Masters/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "query_input = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "embedded_query = embedding_layer(query_input)\n",
    "embedded_query = Flatten()(embedded_query)\n",
    "\n",
    "values = Bidirectional(GRU(10, activation='relu', return_sequences=True))\\\n",
    "                    (embedded_sequences)\n",
    "embedded_query = RepeatVector(MAX_SEQUENCE_LENGTH)(embedded_query)\n",
    "attention_weights = Lambda(dot_prod)([embedded_query, values])\n",
    "attention_weights = Activation('softmax')(attention_weights)\n",
    "context_vector = Lambda(weighted_sum)([attention_weights, values])\n",
    "\n",
    "output = Dense(3, activation='softmax')(context_vector)\n",
    "model = Model(inputs=[sequence_input, query_input], outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optim = Adam()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optim,\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit([x_train[0].reshape(1,1000),\n",
    "           q_train[0]],\n",
    "          y_train[0].reshape(1,2),\n",
    "          batch_size=128,\n",
    "          epochs=30,\n",
    "          validation_data=([x_train[0].reshape(1,1000),\n",
    "                            q_train[0]],\n",
    "                           y_train[0].reshape(1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Masters/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/Masters/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 3306 samples, validate on 826 samples\n",
      "Epoch 1/30\n",
      "3306/3306 [==============================] - 6s 2ms/step - loss: 6.3968 - acc: 0.4111 - val_loss: 1.1908 - val_acc: 0.7337\n",
      "Epoch 2/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 2.4852 - acc: 0.6818 - val_loss: 0.8461 - val_acc: 0.6332\n",
      "Epoch 3/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9957 - acc: 0.6848 - val_loss: 0.7816 - val_acc: 0.7324\n",
      "Epoch 4/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9307 - acc: 0.6984 - val_loss: 0.7693 - val_acc: 0.7312\n",
      "Epoch 5/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9304 - acc: 0.6984 - val_loss: 0.7713 - val_acc: 0.7337\n",
      "Epoch 6/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9276 - acc: 0.6984 - val_loss: 0.7655 - val_acc: 0.7337\n",
      "Epoch 7/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9335 - acc: 0.6984 - val_loss: 0.7654 - val_acc: 0.7337\n",
      "Epoch 8/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9250 - acc: 0.6984 - val_loss: 0.7637 - val_acc: 0.7337\n",
      "Epoch 9/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9171 - acc: 0.6984 - val_loss: 0.7668 - val_acc: 0.7324\n",
      "Epoch 10/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9134 - acc: 0.6984 - val_loss: 0.7620 - val_acc: 0.7337\n",
      "Epoch 11/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9132 - acc: 0.6984 - val_loss: 0.7680 - val_acc: 0.7324\n",
      "Epoch 12/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9115 - acc: 0.6984 - val_loss: 0.7723 - val_acc: 0.7324\n",
      "Epoch 13/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9250 - acc: 0.6981 - val_loss: 0.7608 - val_acc: 0.7337\n",
      "Epoch 14/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9139 - acc: 0.6984 - val_loss: 0.7609 - val_acc: 0.7337\n",
      "Epoch 15/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9047 - acc: 0.6984 - val_loss: 0.7595 - val_acc: 0.7337\n",
      "Epoch 16/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9079 - acc: 0.6981 - val_loss: 0.7583 - val_acc: 0.7337\n",
      "Epoch 17/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9067 - acc: 0.6981 - val_loss: 0.7580 - val_acc: 0.7337\n",
      "Epoch 18/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9056 - acc: 0.6981 - val_loss: 0.7593 - val_acc: 0.7337\n",
      "Epoch 19/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9048 - acc: 0.6984 - val_loss: 0.7601 - val_acc: 0.7337\n",
      "Epoch 20/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9069 - acc: 0.6981 - val_loss: 0.7621 - val_acc: 0.7337\n",
      "Epoch 21/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9537 - acc: 0.6975 - val_loss: 0.7647 - val_acc: 0.7337\n",
      "Epoch 22/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9047 - acc: 0.6981 - val_loss: 0.7620 - val_acc: 0.7312\n",
      "Epoch 23/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.8844 - acc: 0.6981 - val_loss: 0.7659 - val_acc: 0.7312\n",
      "Epoch 24/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.8870 - acc: 0.6978 - val_loss: 0.7588 - val_acc: 0.7324\n",
      "Epoch 25/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.8880 - acc: 0.6975 - val_loss: 0.7553 - val_acc: 0.7324\n",
      "Epoch 26/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.8844 - acc: 0.6972 - val_loss: 0.7608 - val_acc: 0.7312\n",
      "Epoch 27/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9041 - acc: 0.6969 - val_loss: 0.7582 - val_acc: 0.7337\n",
      "Epoch 28/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.9055 - acc: 0.6981 - val_loss: 0.7561 - val_acc: 0.7337\n",
      "Epoch 29/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.8885 - acc: 0.6978 - val_loss: 0.7533 - val_acc: 0.7324\n",
      "Epoch 30/30\n",
      "3306/3306 [==============================] - 4s 1ms/step - loss: 1.8733 - acc: 0.6981 - val_loss: 0.7542 - val_acc: 0.7337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a355b8908>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train, q_train], y_train,\n",
    "          sample_weight = sample_weights_train,\n",
    "          batch_size=128,\n",
    "          epochs=30,\n",
    "          validation_data=([x_val, q_val], y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict([x_val, q_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = y_pred.argmax(axis=1)\n",
    "trues = y_val.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        88\n",
      "           1       0.00      0.00      0.00       132\n",
      "           2       0.73      1.00      0.85       606\n",
      "\n",
      "    accuracy                           0.73       826\n",
      "   macro avg       0.24      0.33      0.28       826\n",
      "weighted avg       0.54      0.73      0.62       826\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Masters/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
