{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, RepeatVector, multiply\n",
    "from keras.layers import Bidirectional, LSTM, MaxPooling1D, Embedding, Flatten\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = '/Users/Masters/Downloads/'\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "EMBEDDING_DIM_DRUGS = 30\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/Masters/Downloads/train_F3WbcTw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unique_hash', 'text', 'drug', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45846 unique tokens.\n",
      "Shape of data tensor: (5279, 1000)\n",
      "Shape of label tensor: (5279, 3)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(df['text'].values)\n",
    "sequences = tokenizer.texts_to_sequences(df['text'].values)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(df['sentiment']))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_query = {drug: i for i, drug in enumerate(list(set(df['drug'].values)))}\n",
    "drug_sequences = np.array([tokenizer_query[drug] for drug in df['drug'].values])\n",
    "drug_sequences = drug_sequences.reshape((len(drug_sequences), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5279, 1) 102\n"
     ]
    }
   ],
   "source": [
    "print(drug_sequences.shape, len(tokenizer_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "drug_sequences = drug_sequences[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "q_train = drug_sequences[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "q_val = drug_sequences[-num_validation_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "embedding_layer_query = Embedding(len(tokenizer_query),\n",
    "                            EMBEDDING_DIM_DRUGS,\n",
    "                            trainable=True,\n",
    "                            input_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1000, 30) (None, 1000, 30)\n"
     ]
    }
   ],
   "source": [
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "query_input = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "embedded_query = embedding_layer_query(query_input)\n",
    "embedded_query = Flatten()(embedded_query)\n",
    "\n",
    "x = Bidirectional(LSTM(15, activation='relu', return_sequences=True))\\\n",
    "                            (embedded_sequences)\n",
    "embedded_query = RepeatVector(MAX_SEQUENCE_LENGTH)(embedded_query)\n",
    "multiplied = multiply([embedded_query, x])\n",
    "output = Dense(1, activation='softmax')(multiplied)\n",
    "output = Flatten()(output)\n",
    "\n",
    "output = Dense(3, activation='softmax')(output)\n",
    "\n",
    "model = Model(inputs=[sequence_input, query_input], outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_29 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 30)        3060        input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 30)           0           embedding_5[11][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1000, 100)    2000100     input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_13 (RepeatVector) (None, 1000, 30)     0           flatten_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_13 (Bidirectional (None, 1000, 30)     13920       embedding_4[11][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_9 (Multiply)           (None, 1000, 30)     0           repeat_vector_13[0][0]           \n",
      "                                                                 bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1000, 1)      31          multiply_9[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 1000)         0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 3)            3003        flatten_12[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 2,020,114\n",
      "Trainable params: 20,014\n",
      "Non-trainable params: 2,000,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Masters/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/Masters/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 4224 samples, validate on 1055 samples\n",
      "Epoch 1/10\n",
      "4224/4224 [==============================] - 47s 11ms/step - loss: 0.8861 - acc: 0.6915 - val_loss: 0.9083 - val_acc: 0.7043\n",
      "Epoch 2/10\n",
      "4224/4224 [==============================] - 43s 10ms/step - loss: 0.8675 - acc: 0.7296 - val_loss: 0.8085 - val_acc: 0.7043\n",
      "Epoch 3/10\n",
      "4224/4224 [==============================] - 42s 10ms/step - loss: 0.8723 - acc: 0.7133 - val_loss: 0.9004 - val_acc: 0.7043\n",
      "Epoch 4/10\n",
      "4224/4224 [==============================] - 42s 10ms/step - loss: 0.8413 - acc: 0.7296 - val_loss: 0.8331 - val_acc: 0.7043\n",
      "Epoch 5/10\n",
      "4224/4224 [==============================] - 43s 10ms/step - loss: 0.8843 - acc: 0.6965 - val_loss: 0.8073 - val_acc: 0.7043\n",
      "Epoch 6/10\n",
      "4224/4224 [==============================] - 43s 10ms/step - loss: 0.8539 - acc: 0.7296 - val_loss: 0.8519 - val_acc: 0.7043\n",
      "Epoch 7/10\n",
      "4224/4224 [==============================] - 42s 10ms/step - loss: 0.8620 - acc: 0.7296 - val_loss: 0.8537 - val_acc: 0.7043\n",
      "Epoch 8/10\n",
      "4224/4224 [==============================] - 42s 10ms/step - loss: 0.8771 - acc: 0.7296 - val_loss: 0.8160 - val_acc: 0.7043\n",
      "Epoch 9/10\n",
      "4224/4224 [==============================] - 43s 10ms/step - loss: 0.8546 - acc: 0.7296 - val_loss: 0.9217 - val_acc: 0.7043\n",
      "Epoch 10/10\n",
      "4224/4224 [==============================] - 45s 11ms/step - loss: 0.8604 - acc: 0.7296 - val_loss: 0.8122 - val_acc: 0.7043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb370a59b0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit([x_train, q_train], y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=([x_val, q_val], y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict([x_val, q_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = y_pred.argmax(axis=1)\n",
    "trues = y_val.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       119\n",
      "           1       0.00      0.00      0.00       193\n",
      "           2       0.70      1.00      0.83       743\n",
      "\n",
      "    accuracy                           0.70      1055\n",
      "   macro avg       0.23      0.33      0.28      1055\n",
      "weighted avg       0.50      0.70      0.58      1055\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Masters/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(trues, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
